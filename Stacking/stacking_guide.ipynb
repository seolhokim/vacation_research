{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. training data를 5개의 test fold로 쪼갬\\n\\n2. train_meta 라는 데이터 셋으로 만듬 training_data로부터. M1,M2 라는 empty columns 만듬\\n유사하게 test_meta만듬. test 로부터. M1,M2 empty columns 만들고.\\n\\n3. 각각의 test fold마다\\n\\n    3.1 4개는 training fold로, 1개는 test fold\\n    \\n    3.2 각각의 clf model마다\\n    \\n        3.2.1 4개로 학습하고 1개로 prediction을 함. prediction은 train_meta에 features로 저장\\n        \\n4. 각각의 모델 fit하고 test 에 test_meta로 저장\\n\\n5. 이과정  거친뒤에 남은 train_meta를 새 clf로 학습시킴\\n\\n6. test_meta에도 동일과정.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stacking\n",
    "#ML Wave , Kaggle's guide\n",
    "#구현하기\n",
    "\n",
    "#기본원리\n",
    "\n",
    "#classifier(regressor) 마다 각자 다른 강점(약점)이 있기 때문에\n",
    "#같이 사용함으로서 정확도를 높일 수 있다.\n",
    "\n",
    "#정확도 70 % 인 clf 3개 사용하면 \n",
    "#34 % + 44 % = 77 % 의 정확도를 보여줄 수 있음.\n",
    "\n",
    "#Kaggle guide\n",
    "\n",
    "'''\n",
    "1. Partition the training data into five test folds\n",
    "\n",
    "2. Create a dataset called train_meta with the same row Ids and fold Ids as the training dataset,\n",
    "with empty columns M1 and M2. Similarly create a dataset called test_meta \n",
    "with the same row Ids as the test dataset and empty columns M1 and M2\n",
    "\n",
    "3. For each test fold\n",
    "    3.1 Combine the other four folds to be used as a training fold\n",
    "    \n",
    "    3.2 For each base model \n",
    "    \n",
    "        3.2.1 Fit the base model to the training fold and make predictions on the test fold.\n",
    "        Store these predictions in train_meta to be used as features for the stacking model\n",
    "\n",
    "4. Fit each base model to the full training dataset and make predictions on the test dataset.\n",
    "Store these predictions inside test_meta\n",
    "\n",
    "5. Fit a new model, S (i.e the stacking model) to train_meta,\n",
    "using M1 and M2 as features. Optionally,\n",
    "include other features from the original training dataset or engineered features\n",
    "\n",
    "6. Use the stacked model S to make final predictions on test_meta\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "1. training data를 5개의 test fold로 쪼갬\n",
    "\n",
    "2. train_meta 라는 데이터 셋으로 만듬 training_data로부터. M1,M2 라는 empty columns 만듬\n",
    "유사하게 test_meta만듬. test 로부터. M1,M2 empty columns 만들고.\n",
    "\n",
    "3. 각각의 test fold마다\n",
    "\n",
    "    3.1 4개는 training fold로, 1개는 test fold\n",
    "    \n",
    "    3.2 각각의 clf model마다\n",
    "    \n",
    "        3.2.1 4개로 학습하고 1개로 prediction을 함. prediction은 train_meta에 features로 저장\n",
    "        \n",
    "4. 각각의 모델 fit하고 test 에 test_meta로 저장\n",
    "\n",
    "5. 이과정  거친뒤에 남은 train_meta를 새 clf로 학습시킴\n",
    "\n",
    "6. test_meta에도 동일과정.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "import xgboost as xgb\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle_cv(model, n_folds=5):\n",
    "    kf = KFold(n_folds, shuffle=True).get_n_splits(scale_data)\n",
    "    rmse= np.sqrt(-cross_val_score(model, scale_data,logged_train_label, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.09,\n",
    "                             learning_rate=0.009, \n",
    "                              n_estimators=100,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213,\n",
    "                             random_state =7, nthread = -1)\n",
    "\n",
    "xgb_m = xgb.XGBRegressor(leraning_rate = 0.05, n_estimators= 1000)\n",
    "\n",
    "GBoost = GradientBoostingRegressor(max_depth=2, learning_rate=0.005,min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "\n",
    "KRR = KernelRidge(alpha=0.1, kernel='polynomial', degree=1, coef0=5)\n",
    "\n",
    "ENet = ElasticNet(alpha=0.005, l1_ratio=.6, random_state=3)\n",
    "\n",
    "lasso = Lasso(alpha =0.005, random_state=1)\n",
    "\n",
    "RForest = RandomForestRegressor(n_estimators=100,max_depth=2,min_samples_leaf=2\\\n",
    "                               ,min_weight_fraction_leaf=0.5,max_leaf_nodes=2\\\n",
    "                                ,random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for i, model in enumerate(self.base_models):\n",
    "            \n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                \n",
    "                self.base_models_[i].append(instance)\n",
    "                \n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "      \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        \n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet,KRR,lasso,RForest),meta_model =  GBoost)\n",
    "rmsle_cv(stacked_averaged_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_models = (ENet,lasso,KRR,model_xgb),meta_model =  GBoost 0.053922348347839634\n",
    "#base_models = (ENet,lasso,KRR,model_xgb),meta_model =  GBoost 0.054324193590506976\n",
    "#base_models = (ENet,KRR),meta_model =  GBoost 0.0537208976739747"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
