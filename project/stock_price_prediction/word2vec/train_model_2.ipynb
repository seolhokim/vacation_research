{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델을 짤거임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Dropout, Activation, Add,Bidirectional,LSTM,MaxPooling1D, Conv1D,BatchNormalization, Flatten, Dense,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ie-45\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import glob\n",
    "from konlpy.tag import Twitter \n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data // \n",
    "test_data //"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input \n",
    "embedding_layer\n",
    "conv2d()\n",
    "lstm()\n",
    "lstm()\n",
    "lstm()\n",
    "dense()\n",
    "dense()\n",
    "Model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = Word2Vec.load(\"./model/last_train_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ie-45\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, embedding_size = pretrained_weights.shape\n",
    "max_sequence_length = 2000 #train 최대 길이에 맞추고 test는 그길이보다 길면 잘라줌\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ie-45\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(26252, 200)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26252"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#첫모델 아직 input 안됨\n",
    "def model_creation(): \n",
    "    input_layer = Input(shape=(max_sequence_length,))\n",
    "    embedding_layer = Embedding(vocab_size,embedding_size,\n",
    "        weights=[word_model.wv.syn0],\n",
    "        input_length=max_sequence_length,trainable=False)(input_layer) \n",
    "    \n",
    "    #embedding_layer_2 = np.expand_dims(X, axis=2)\n",
    "    c0 = Dropout(0.2)(embedding_layer)\n",
    "    c1 = (Conv1D(64,kernel_size = 5, border_mode='same'))(c0)\n",
    "    c2 = (BatchNormalization())(c1)\n",
    "    c3 = (Activation('relu'))(c2)\n",
    "    c4 = (MaxPooling1D(pool_size=(4)))(c3)\n",
    "    c5 = (Dropout(0.25))(c4)\n",
    "\n",
    "    c6 = (Conv1D(64,3, border_mode='same'))(c5)\n",
    "    c7 = (BatchNormalization())(c6)\n",
    "    c8 = (Activation('relu'))(c7)\n",
    "    c9 = (MaxPooling1D(pool_size=(4)))(c8)\n",
    "    c10 = (Dropout(0.25))(c9)\n",
    "\n",
    "    left1 = Bidirectional(LSTM(output_dim=256, init='uniform', inner_init='uniform',\n",
    "                   forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "                   inner_activation='sigmoid', input_shape=(32, 32)))(c10)\n",
    "    left2 = Bidirectional(LSTM(output_dim=256, init='uniform', inner_init='uniform',\n",
    "                   forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "                   inner_activation='sigmoid', input_shape=(32, 32)))(left1)\n",
    "\n",
    "    right1 =Bidirectional(LSTM(output_dim=256, init='uniform', inner_init='uniform',\n",
    "               forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "               inner_activation='sigmoid', input_shape=(32, 32), go_backwards=True))(c10)\n",
    "    right2 =Bidirectional(LSTM(output_dim=256, init='uniform', inner_init='uniform',\n",
    "               forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "               inner_activation='sigmoid', input_shape=(32, 32), go_backwards=True))(right1)\n",
    "    \n",
    "    \n",
    "    merged = Add()([left2,right2])\n",
    "\n",
    "    h1 = (Flatten())(merged)\n",
    "    h2 = (Dense(512,activation='relu'))(h1)\n",
    "    h3 = (Dense(256,activation='relu'))(h2)\n",
    "    h4 = (Dense(128,activation='relu'))(h3)\n",
    "\n",
    "    h5 = (Dense(2,activation='sigmoid'))(h4)\n",
    "\n",
    "\n",
    "    model = Model(inputs = input_layer , outputs = output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creation(): \n",
    "    input_layer = Input(shape=(max_sequence_length,))\n",
    "    embedding_layer = Embedding(vocab_size,embedding_size,\n",
    "        weights=[word_model.wv.syn0],\n",
    "        input_length=max_sequence_length,trainable=False)(input_layer) \n",
    "    \n",
    "    \n",
    "    left1 = Bidirectional(LSTM(output_dim=256, init='uniform', inner_init='uniform',\n",
    "                   forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "                   inner_activation='sigmoid', input_shape=(32, 32)))(embedding_layer)\n",
    "    left2 = Bidirectional(LSTM(output_dim=256, init='uniform', inner_init='uniform',\n",
    "                   forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "                   inner_activation='sigmoid', input_shape=(32, 32)))(left1)\n",
    "\n",
    "    right1 =Bidirectional(LSTM(output_dim=256, init='uniform', inner_init='uniform',\n",
    "               forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "               inner_activation='sigmoid', input_shape=(32, 32), go_backwards=True))(embedding_layer)\n",
    "    right2 =Bidirectional(LSTM(output_dim=256, init='uniform', inner_init='uniform',\n",
    "               forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "               inner_activation='sigmoid', input_shape=(32, 32), go_backwards=True))(right1)\n",
    "    #embedding_layer_2 = np.expand_dims(X, axis=2)\n",
    "    merged = Add()([left2,right2])\n",
    "    \n",
    "    c0 = Dropout(0.2)(merged)\n",
    "    c1 = (Conv1D(64,kernel_size = 5, border_mode='same'))(c0)\n",
    "    c2 = (BatchNormalization())(c1)\n",
    "    c3 = (Activation('relu'))(c2)\n",
    "    c4 = (MaxPooling1D(pool_size=(4)))(c3)\n",
    "    c5 = (Dropout(0.25))(c4)\n",
    "\n",
    "    c6 = (Conv1D(64,3, border_mode='same'))(c5)\n",
    "    c7 = (BatchNormalization())(c6)\n",
    "    c8 = (Activation('relu'))(c7)\n",
    "    c9 = (MaxPooling1D(pool_size=(4)))(c8)\n",
    "    c10 = (Dropout(0.25))(c9)\n",
    "\n",
    "    h1 = (Flatten())(c10)\n",
    "    h2 = (Dense(512,activation='relu'))(h1)\n",
    "    h3 = (Dense(256,activation='relu'))(h2)\n",
    "    h4 = (Dense(128,activation='relu'))(h3)\n",
    "\n",
    "    output = (Dense(2,activation='sigmoid'))(h4)\n",
    "\n",
    "\n",
    "    model = Model(inputs = input_layer , outputs = output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ie-45\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\ie-45\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=True, activation=\"tanh\", input_shape=(32, 32), unit_forget_bias=True, units=256, kernel_initializer=\"uniform\", recurrent_initializer=\"uniform\", recurrent_activation=\"sigmoid\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\ie-45\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=True, activation=\"tanh\", input_shape=(32, 32), unit_forget_bias=True, units=256, kernel_initializer=\"uniform\", recurrent_initializer=\"uniform\", recurrent_activation=\"sigmoid\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\ie-45\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=True, activation=\"tanh\", input_shape=(32, 32), go_backwards=True, unit_forget_bias=True, units=256, kernel_initializer=\"uniform\", recurrent_initializer=\"uniform\", recurrent_activation=\"sigmoid\")`\n",
      "C:\\Users\\ie-45\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=True, activation=\"tanh\", input_shape=(32, 32), go_backwards=True, unit_forget_bias=True, units=256, kernel_initializer=\"uniform\", recurrent_initializer=\"uniform\", recurrent_activation=\"sigmoid\")`\n",
      "C:\\Users\\ie-45\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, kernel_size=5, padding=\"same\")`\n",
      "C:\\Users\\ie-45\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, padding=\"same\")`\n"
     ]
    }
   ],
   "source": [
    "model = model_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging(one_row):\n",
    "    hangle = re.compile('[^ ㄱ-ㅣ가-힣0-9]+')\n",
    "    result = hangle.sub('',one_row)\n",
    "    result = tagger.pos(result,norm=True,stem = True)\n",
    "    return result\n",
    "\n",
    "def data_preprocessing(one_row):\n",
    "    data = tagging(one_row)\n",
    "    r = []\n",
    "    results = []\n",
    "    jung_bok = \"\"\n",
    "    for (word, pumsa) in data:\n",
    "        if not pumsa in [\"Josa\", \"Eomi\", \"Punctuation\",'Number'] and jung_bok !=word:\n",
    "            r.append(word)\n",
    "            jung_bok = word\n",
    "    results.append((\" \".join(r)).strip())  \n",
    "    return results[0]\n",
    "\n",
    "def data_making(input_data):\n",
    "    data = pd.read_csv(input_data,encoding='euc-kr').iloc[:,1:]\n",
    "    data['times'] =data['times'].str[:10]\n",
    "    data['times'] = data['times'].apply(lambda x : x.replace('.',\"-\"))\n",
    "    data['str'] = data['titles'].str[:]+data['articles']\n",
    "    del data['titles']\n",
    "    del data['articles']\n",
    "    data['str'] = data['str'].apply(data_preprocessing)\n",
    "    return data\n",
    "def dataframe_preprocessing(data,i,d):\n",
    "    dataframe = data_making(data)\n",
    "    data = dataframe['str'][i].split()\n",
    "    data = word_to_vec(data)\n",
    "    return data,dataframe[d][i]\n",
    "    \n",
    "def word_to_vec(data):\n",
    "    word_vec_list = []\n",
    "    for txt in data:\n",
    "        try:\n",
    "            vec = word_model.wv[txt]\n",
    "            word_vec_list.append(vec)\n",
    "        except:\n",
    "            pass\n",
    "    return word_vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = glob.glob(\"./train_data/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./train_data\\\\2017.06.01_2017.12.31_hmscience.csv'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_dir[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data\n",
    "train_one_row,train_one_label = dataframe_preprocessing(train_data_dir[0],0,'d1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1_d1_label = train_data_1['d1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = data_preprocessing(train_data_1.iloc[0,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
