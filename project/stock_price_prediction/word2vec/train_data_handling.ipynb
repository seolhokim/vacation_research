{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "#csv파일 받아서 형태소화 시키고 csv파일로만들어줄거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import glob\n",
    "from konlpy.tag import Twitter \n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = Twitter()\n",
    "\n",
    "def data_read(data_place):\n",
    "\n",
    "    f = open(data_place,'r')\n",
    "    txt = f.read()\n",
    "\n",
    "    hangle = re.compile('[^ ㄱ-ㅣ가-힣0-9]+')\n",
    "    result = hangle.sub('',txt)\n",
    "    #print(\"result clear\")\n",
    "    result = tagger.pos(result,norm=True,stem = True)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def train_data_make(data):\n",
    "    r = []\n",
    "    results = []\n",
    "    jung_bok = \"\"\n",
    "    for (word, pumsa) in data:\n",
    "        if not pumsa in [\"Josa\", \"Eomi\", \"Punctuation\"] and jung_bok !=word:\n",
    "            r.append(word)\n",
    "            jung_bok = word\n",
    "    results.append((\" \".join(r)).strip())  \n",
    "    return results\n",
    "def train_data_to_txt(results,name = ''):\n",
    "    with open(name,'w') as fp:\n",
    "        fp.write(results)\n",
    "        \n",
    "def train_data_creation(data_dir,data_name):\n",
    "    for i in range(len(data_name)):\n",
    "        result = data_read(data_name[i])\n",
    "        result = train_data_make(result)\n",
    "        dir_name = data_dir+str(i)+'.txt'\n",
    "        train_data_to_txt(result[0],dir_name)\n",
    "        print(i,\"/\",len(data_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging(one_row):\n",
    "    hangle = re.compile('[^ ㄱ-ㅣ가-힣0-9]+')\n",
    "    result = hangle.sub('',one_row)\n",
    "    result = tagger.pos(result,norm=True,stem = True)\n",
    "    return result\n",
    "\n",
    "def data_preprocessing(one_row):\n",
    "    data = tagging(one_row)\n",
    "    r = []\n",
    "    results = []\n",
    "    jung_bok = \"\"\n",
    "    for (word, pumsa) in data:\n",
    "        if not pumsa in [\"Josa\", \"Eomi\", \"Punctuation\",'Number'] and jung_bok !=word:\n",
    "            r.append(word)\n",
    "            jung_bok = word\n",
    "    results.append((\" \".join(r)).strip())  \n",
    "    return results[0]\n",
    "\n",
    "def data_making(input_data):\n",
    "    data = pd.read_csv(input_data,encoding='euc-kr').iloc[:,1:]\n",
    "    data['times'] =data['times'].str[:10]\n",
    "    data['times'] = data['times'].apply(lambda x : x.replace('.',\"-\"))\n",
    "    data['str'] = data['titles'].str[:]+data['articles']\n",
    "    del data['titles']\n",
    "    del data['articles']\n",
    "    data['str'] = data['str'].apply(data_preprocessing)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"./model/last_train_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_data = glob.glob(\"important/male/csv/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "korea_data = glob.glob(\"./important/korea/csv/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_preprocessing(data,i):\n",
    "    data = data_making(data)\n",
    "    #print(\"data's shape\",data.shape)\n",
    "    data = data.iloc[i,1].split()\n",
    "    data = word_to_vec(data)\n",
    "    return data\n",
    "    \n",
    "def word_to_vec(data):\n",
    "    word_vec_list = []\n",
    "    for txt in data:\n",
    "        try:\n",
    "            vec = model.wv[txt]\n",
    "            word_vec_list.append(vec)\n",
    "        except:\n",
    "            pass\n",
    "    return word_vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataframe_preprocessing(male_data[1],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1722\n"
     ]
    }
   ],
   "source": [
    "for i in range(106):\n",
    "    if( max_len < len(dataframe_preprocessing(male_data[1],i))):\n",
    "        max_len = len(dataframe_preprocessing(male_data[1],i))\n",
    "print(max_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
