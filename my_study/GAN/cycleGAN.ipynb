{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/eriklindernoren/Keras-GAN/blob/master/cyclegan/cyclegan.py\n",
    "\n",
    "#참고하였음.\n",
    "#dataset 주소\n",
    "\n",
    "#https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainA_dir = \"./horse2zebra/trainA/*.jpg\"\n",
    "trainB_dir = \"./horse2zebra/trainB/*.jpg\"\n",
    "testA_dir = \"./horse2zebra/testA/*.jpg\"\n",
    "testB_dir = \"./horse2zebra/testB/*.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN:\n",
    "    def __init__(self):\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "    \n",
    "        self.generator_filter = 32\n",
    "        self.discriminator_filter = 64\n",
    "        \n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "        self.lambda_cycle = 10.0     \n",
    "        self.lambda_id = 0.1 * self.lambda_cycle\n",
    "        \n",
    "        self.optimizer = Adam(0.0002, 0.5)\n",
    "        \n",
    "        self.d_A = self.build_discriminator()\n",
    "        self.d_B = self.build_discriminator()\n",
    "        \n",
    "        self.d_A.compile(loss = 'mse', optimizer = self.optimizer ,metrics = ['accuracy'])\n",
    "        self.d_B.compile(loss = 'mse', optimizer = self.optimizer ,metrics = ['accuracy'])\n",
    "    \n",
    "        self.g_AB = self.build_generator()\n",
    "        self.g_BA = self.build_generator()\n",
    "        \n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "        \n",
    "        fake_B = self.g_AB(img_A)\n",
    "        fake_A = self.g_BA(img_B)\n",
    "        \n",
    "        reconstr_A = self.g_BA(fake_B)\n",
    "        reconstr_B = self.g_AB(fake_A)\n",
    "        \n",
    "        img_A_id = self.g_BA(img_A)\n",
    "        img_B_id = self.g_AB(img_B)\n",
    "        \n",
    "        \n",
    "        valid_A = self.d_A(fake_A)\n",
    "        valid_B = self.d_B(fake_B)\n",
    "        \n",
    "        self.cyclegan = Model(inputs  = [img_A,img_B],outputs = [valid_A,valid_B,\\\n",
    "                                                               reconstr_A,reconstr_B,\\\n",
    "                                                               img_A_id,img_B_id])\n",
    "        self.cyclegan.compile(loss=['mse','mse','mae','mae','mae','mae'],optimizer =\\\n",
    "                             self.optimizer,loss_weights= [1,1,self.lambda_cycle,\\\n",
    "                                                          self.lambda_cycle,\\\n",
    "                                                          self.lambda_id, self.lambda_id] )\n",
    "        self.d_A.trainable = False\n",
    "        self.d_B.trainable = False\n",
    "        \n",
    "    \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if normalization:\n",
    "                d = BatchNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        d1 = d_layer(img, self.discriminator_filter, normalization=False)\n",
    "        d2 = d_layer(d1, self.discriminator_filter*2)\n",
    "        d3 = d_layer(d2, self.discriminator_filter*4)\n",
    "        d4 = d_layer(d3, self.discriminator_filter*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model(img, validity)\n",
    "    \n",
    "    def build_generator(self):\n",
    "        def conv2d(layer_input, filters, f_size=4):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = BatchNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization()(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "            # Downsampling\n",
    "        d1 = conv2d(d0, self.generator_filter)\n",
    "        d2 = conv2d(d1, self.generator_filter*2)\n",
    "        d3 = conv2d(d2, self.generator_filter*4)\n",
    "        d4 = conv2d(d3, self.generator_filter*8)\n",
    "\n",
    "            # Upsampling\n",
    "        u1 = deconv2d(d4, d3, self.generator_filter*4)\n",
    "        u2 = deconv2d(u1, d2, self.generator_filter*2)\n",
    "        u3 = deconv2d(u2, d1, self.generator_filter)\n",
    "\n",
    "        u4 = UpSampling2D(size=2)(u3)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
    "        \n",
    "        return Model(d0, output_img)\n",
    "    \n",
    "    def train(self, epochs, batch_size = 1):\n",
    "    \n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        \n",
    "        trainA = self.image_name(trainA_dir)\n",
    "        trainB = self.image_name(trainB_dir)\n",
    "        for epoch in range(epochs):\n",
    "            train_len = len(trainA)\n",
    "            \n",
    "            for i in range(train_len):\n",
    "                imgs_A = self.image_loading(trainA[i])\n",
    "                imgs_B = self.image_loading(trainB[i])\n",
    "            \n",
    "                \n",
    "                fake_B = self.g_AB.predict(imgs_A)\n",
    "                print(\"fake_B\")\n",
    "                fake_A = self.g_BA.predict(imgs_B)\n",
    "                print(\"fake_A\")\n",
    "                \n",
    "                self.d_A.train_on_batch(imgs_A,valid)\n",
    "                print(\"d_A train valid\")\n",
    "                self.d_A.train_on_batch(fake_A, fake)\n",
    "                print(\"d_A train fake\")\n",
    "                \n",
    "                self.d_B.train_on_batch(imgs_B,valid)\n",
    "                print(\"d_B train valid\")\n",
    "                self.d_B.train_on_batch(fake_B, fake)\n",
    "                print(\"d_B train fake\")\n",
    "                \n",
    "                self.cyclegan.train_on_batch([imgs_A,imgs_B],\\\n",
    "                                            [valid,valid,\\\n",
    "                                            imgs_A, imgs_B,\\\n",
    "                                            imgs_A, imgs_B])\n",
    "                \n",
    "                print(i,\"/\", train_len)\n",
    "            print(epoch,\"/\",epochs)\n",
    "            \n",
    "    def output(self,num = 0):\n",
    "        r,c = 2,3\n",
    "        testA = self.image_name(testA_dir)\n",
    "        testB = self.image_name(testB_dir)\n",
    "        \n",
    "        imgs_A = self.image_loading(testA[num])\n",
    "        imgs_B = self.image_loading(testB[num])\n",
    "        \n",
    "        fake_B = self.g_AB.predict(imgs_A)\n",
    "        fake_A = self.g_BA.predict(imgs_B)\n",
    "        \n",
    "        reconstr_A = self.g_BA.predict(fake_B)\n",
    "        reconstr_B = self.g_AB.predict(fake_A)\n",
    "        \n",
    "        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Original', 'Translated', 'Reconstructed']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[j])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "    \n",
    "    def image_name(self,file_place):\n",
    "        file_names = glob.glob(file_place)\n",
    "        return file_names\n",
    "    \n",
    "    def image_loading(self,im_name):\n",
    "        im = Image.open(im_name).convert('RGB')\n",
    "        im = im.resize((self.img_rows,self.img_cols),Image.BILINEAR)\n",
    "        im = np.array(im)\n",
    "        im = im.reshape((-1,self.img_rows,self.img_cols,self.channels))\n",
    "        return im\n",
    "\n",
    "cyclegan_model = CycleGAN()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_B\n",
      "fake_A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sh2\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "0 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "1 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "2 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "3 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "4 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "5 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "6 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "7 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "8 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "9 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "10 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "11 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "12 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "13 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "14 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "15 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "16 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "17 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "18 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "19 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "20 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "21 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "22 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "23 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "24 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "25 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "26 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "27 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "28 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "29 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "30 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "31 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "32 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "33 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "34 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "35 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "36 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "37 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "38 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "39 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "40 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n",
      "41 / 1067\n",
      "fake_B\n",
      "fake_A\n",
      "d_A train valid\n",
      "d_A train fake\n",
      "d_B train valid\n",
      "d_B train fake\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-df2e7aa5df91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcyclegan_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-93-095179761edb>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, batch_size)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"d_B train fake\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcyclegan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimgs_A\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimgs_B\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                                            \u001b[1;33m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m                                            \u001b[0mimgs_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs_B\u001b[0m\u001b[1;33m,\u001b[0m                                            \u001b[0mimgs_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs_B\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1888\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1890\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1891\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1892\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cyclegan_model.train(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_name(file_place):\n",
    "    file_names = glob.glob(file_place)\n",
    "    return file_names\n",
    "    \n",
    "def image_loading(im_name):\n",
    "    im = Image.open(im_name).convert('RGB')\n",
    "    im = im.resize((128,128),Image.BILINEAR)\n",
    "    im = np.array(im)\n",
    "    im = im.reshape(-1,128,128,3)\n",
    "    return im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_loading(image_name(trainA_dir)[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train ,_),(_,_) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
