{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#참고\n",
    "\n",
    "#https://github.com/jaara/AI-blog/blob/master/CartPole-A3C.py 의 코드를\n",
    "#보면서 첨삭하는 식으로 짰습니다.\n",
    "\n",
    "#공부한 사이트\n",
    "'''\n",
    "https://jaromiru.com/2017/03/26/lets-make-an-a3c-implementation/\n",
    "https://www.youtube.com/watch?v=gINks-YCTBs&t=3531s\n",
    "https://github.com/XenderLiu/Policy-Gradient-and-Actor-Critic-Keras/blob/master/agent_dir/agent_actorcritic.py\n",
    "https://github.com/rlcode/reinforcement-learning-kr/blob/master/2-cartpole/2-actor-critic/cartpole_a2c.py#L79\n",
    "http://www.modulabs.co.kr/RL_library/3305\n",
    "'''\n",
    "\n",
    "#gradient관련\n",
    "\n",
    "'''\n",
    "https://wikidocs.net/8281\n",
    "http://blog.naver.com/PostView.nhn?blogId=atelierjpro&logNo=220978848453&parentCategoryNo=&categoryNo=&viewDate=&isShowPopularPosts=false&from=postView\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.layers import Dense,merge\n",
    "from keras.models import Sequential ,Input, Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "N_STEP_RETURN = 8\n",
    "GAMMA_N = GAMMA ** N_STEP_RETURN\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "reward_size = 1\n",
    "\n",
    "LOSS_V = .5 # v loss coefficient\n",
    "LOSS_ENTROPY = .01 # entropy coefficient\n",
    "MIN_BATCH = 32\n",
    "NONE_STATE = np.zeros(state_size)\n",
    "name = 'CartPole-v1'\n",
    "\n",
    "class Agent:\n",
    "    #environment와 상호작용으로 얻은 data를 memory에 담고\n",
    "    #처리해 brain으로 push\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "        \n",
    "        self.epsilon = 1\n",
    "        self.decaying_per = 0.999\n",
    "        \n",
    "        self.R = 0 #이후에 받을수 있는 모든 Reward 는 R, 현상황에 받는 reward는 reward로 표기하였다.\n",
    "        \n",
    "    def getEpsilon(self):\n",
    "        self.epsilon *= self.decaying_per #expliot and exploration\n",
    "        return self.epsilon\n",
    "        \n",
    "    def act(self,state,actor):\n",
    "        #decaying e를 통해 expliot and exploration는 하지만\n",
    "        #목적함수에 cross entropy로 exploration이 일어나도록 했는데, 또 해야하는지는 의문\n",
    "        #random.choice로 주는 것도 확률을 두번일으키게 하는 것이라, 수정할까 고민함.\n",
    "        if(random.random() > self.getEpsilon()):\n",
    "            return random.randint(0,action_size-1)\n",
    "        else :\n",
    "            state = np.array([state])\n",
    "            \n",
    "            percent = actor.predict_action(state)\n",
    "            action = np.random.choice(action_size,p=percent[0])\n",
    "            return action\n",
    "    \n",
    "    def train(self, state, action, reward, next_state):\n",
    "        #한번 행동후 sars 모두 memory에 저장,\n",
    "        #Reward 계산.\n",
    "        #game이 done됐을시 get_sample 함수를 통해\n",
    "        #next_state를 수정하며,Reward 계산 후 brain으로 현재 쌓인학습데이터를 모두 보냄\n",
    "        \n",
    "        # N_STEP_RETURN보다 memory의 길이가 길어졌을 시,\n",
    "        # next_state 수정하며,Reward 계산 후, memory 비움\n",
    "        def get_sample(memory,n):\n",
    "            state, action, _, _ =memory[0]\n",
    "            _, _, _, next_state = memory[n-1]\n",
    "            return state, action, self.R, next_state\n",
    "        \n",
    "        action_onehot = np.zeros(action_size)\n",
    "        action_onehot[action] = 1\n",
    "        \n",
    "        self.memory.append((state,action_onehot,reward,next_state))\n",
    "        \n",
    "        self.R = (self.R + reward * GAMMA_N) / GAMMA\n",
    "        \n",
    "        if next_state is None :\n",
    "            \n",
    "            while len(self.memory) > 0 :\n",
    "                n = len(self.memory)\n",
    "                state, action, reward, next_state = get_sample(self.memory,n)\n",
    "                brain.train_push(state, action, reward, next_state)\n",
    "                \n",
    "                self.R = (self.R - self.memory[0][2]) / GAMMA\n",
    "                self.memory.pop(0)\n",
    "                \n",
    "            self.R = 0\n",
    "            \n",
    "        if len(self.memory) >= N_STEP_RETURN : \n",
    "            state, action, reward, next_state = get_sample(self.memory, N_STEP_RETURN)\n",
    "            brain.train_push(state, action, reward, next_state)\n",
    "            \n",
    "            self.R = self.R - self.memory[0][2]\n",
    "            self.memory.pop(0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Brain:\n",
    "    '''\n",
    "    각 Environment의 agents 로부터 정보를 받아 한꺼번에 처리함으로서 학습데이터간의 종속성 해결\n",
    "    tensorflow session은 오류검색해서 session등록만 해주고 다 backend K로 통일해 함수사용했음.\n",
    "    \n",
    "    '''\n",
    "    #train_queue는 각 Environment의 agent의 state, action, reward, next_state, next_state의 상태를 담음\n",
    "    train_queue = [ [], [], [], [], [] ]\n",
    "    lock_queue = threading.Lock()\n",
    "    def __init__(self):\n",
    "        #먼저 기존 tensorflow 에서 세션을 만든다. \n",
    "        #그리고 그것을 keras에 등록시켜 keras가 해당 세션을 사용할 수 있도록 한다.\n",
    "        self.session = tf.Session()\n",
    "        K.set_session(self.session)\n",
    "        \n",
    "        #직접 만든 tensor를 쓰기위해선 initialization을 해야함\n",
    "        K.manual_variable_initialization(True)\n",
    "        self.actor_lr = 0.003\n",
    "        \n",
    "        #policy 를 actor_model을 통해 근사시킴\n",
    "        self.model = self.actor_model()\n",
    "        _,_,_,self.object_function = self.build_object_function(self.model)\n",
    "        \n",
    "        #변수를 만들었기 때문에 initializer를 run해줘야함\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.default_graph = tf.get_default_graph()\n",
    "        \n",
    "    def load_model(self,dir):\n",
    "        self.model.load_weights(dir)\n",
    "        \n",
    "    def save_model(self,dir):\n",
    "        self.model.save_weights(dir)\n",
    "        \n",
    "    def actor_model(self):\n",
    "        #actor의 policy를 근사하는 output action과\n",
    "        #critic역할을 하는 action_value를 return\n",
    "        input_layer = Input(batch_shape = (None,state_size))\n",
    "        h1 = Dense(32, activation = 'relu')(input_layer)\n",
    "        action = Dense(action_size,activation = 'softmax')(h1)\n",
    "        action_value = Dense(1,activation='linear')(h1)\n",
    "        model = Model(inputs = input_layer , outputs=[action,action_value])\n",
    "        return model\n",
    "    \n",
    "    def build_object_function(self,model):\n",
    "        #placeholder 생성\n",
    "        state_tensor = K.placeholder(dtype = 'float32', shape = (None,state_size))\n",
    "        action_tensor = K.placeholder(dtype = 'float32',shape = (None,action_size))\n",
    "        reward_tensor = K.placeholder(dtype = 'float32',shape=(None,reward_size))\n",
    "        #model로부터 action확률과, value를 tensor로 받아옴.\n",
    "        probability, value = model(state_tensor)\n",
    "        #probability가 0이 될까봐 1e-10을 더함\n",
    "        log_probability = K.log(K.sum(probability * action_tensor\\\n",
    "                                             ,keepdims=True)+1e-10)\n",
    "        #reward - base value로 분산을 낮춰줌\n",
    "        advantage = reward_tensor - value\n",
    "        #policy 를 근사할땐 advantage gradient의 update는 멈춰줘야함\n",
    "        loss_policy = - log_probability * K.stop_gradient(advantage)\n",
    "        #value는 advantage값의 ^2에 계수를 곱\n",
    "        loss_value = K.constant(LOSS_V) * K.square(advantage)\n",
    "        #exploration 을위해 entropy\n",
    "        entropy = K.constant(LOSS_ENTROPY) * K.sum(probability * K.log(probability+1e-10),\\\n",
    "                                             axis = 1, keepdims = True)\n",
    "        #최종 loss function\n",
    "        loss_total = K.mean(loss_policy + loss_value + entropy)\n",
    "        \n",
    "        optimizer = RMSprop(lr=self.actor_lr, decay=.99)\n",
    "        updates = optimizer.get_updates(model.trainable_weights,[],loss_total)\n",
    "        train = K.function([state_tensor,action_tensor,reward_tensor],[],updates=updates)\n",
    "        return state_tensor, action_tensor, reward_tensor, train\n",
    "    \n",
    "    def optimize(self):\n",
    "        #MIN_BATCH 보다 train_queue가 작으면 return\n",
    "        #MIN_BATCH* 5 보다 처리해야할 양이 많으면 들어오는 batch를 줄이던지,\n",
    "        #optimizer갯수를늘림\n",
    "        #object_function 으로 보냄\n",
    "        if len(self.train_queue[0])< MIN_BATCH:\n",
    "            return\n",
    "        with self.lock_queue:\n",
    "            if len(self.train_queue[0]) < MIN_BATCH:# more thread could have passed without lock\n",
    "                return \n",
    "        \n",
    "            state, action, reward, next_state, state_mask = self.train_queue\n",
    "            self.train_queue = [[],[],[],[],[]]\n",
    "        #학습데이터를 np.vstack으로 쌓음\n",
    "        state = np.vstack(state)\n",
    "        action = np.vstack(action)\n",
    "        reward = np.vstack(reward)\n",
    "        next_state = np.vstack(next_state)\n",
    "        state_mask = np.vstack(state_mask)\n",
    "        \n",
    "        if len(state) > 5 * MIN_BATCH:\n",
    "            print(\"minimizing batch\")\n",
    "        # train data로부터는 N_STEP_RETURN 이후의 value를 못받으므로 더해줘야함\n",
    "        value = self.predict_value(next_state)\n",
    "        \n",
    "        reward = reward + GAMMA_N * value * state_mask\n",
    "        self.object_function([state,action,reward])\n",
    "    \n",
    "    \n",
    "    def train_push(self,state, action, reward, next_state):\n",
    "        #train_queue로 학습데이터를 보냄\n",
    "        with self.lock_queue:\n",
    "            self.train_queue[0].append(state)\n",
    "            self.train_queue[1].append(action)\n",
    "            self.train_queue[2].append(reward)\n",
    "\n",
    "            if next_state is None:\n",
    "                self.train_queue[3].append(NONE_STATE)\n",
    "                self.train_queue[4].append(0.)\n",
    "\n",
    "            else :\n",
    "                self.train_queue[3].append(next_state)\n",
    "                self.train_queue[4].append(1.)\n",
    "\n",
    "    def predict(self, state): \n",
    "        with self.default_graph.as_default():\n",
    "            action, value = self.model.predict(state)\n",
    "            return action,value\n",
    "\n",
    "    def predict_action(self, state): \n",
    "        with self.default_graph.as_default():\n",
    "            action, value = self.model.predict(state)\n",
    "            return action\n",
    "\n",
    "    def predict_value(self, state): \n",
    "        with self.default_graph.as_default():\n",
    "            action, value = self.model.predict(state)\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Environment(threading.Thread):\n",
    "    #환경을 thread로 여러개돌리기위해 class로만듬\n",
    "    stop_signal = False\n",
    "    \n",
    "    def __init__(self,game_name,render = False):\n",
    "        threading.Thread.__init__(self)\n",
    "        \n",
    "        self.render = render\n",
    "        self.env = gym.make(game_name)\n",
    "        self.agent = Agent()\n",
    "        \n",
    "    def run_episode(self):\n",
    "        state = self.env.reset()\n",
    "        R = 0\n",
    "\n",
    "        while True:\n",
    "            if self.render :\n",
    "                self.env.render()\n",
    "            action = self.agent.act(state,brain)\n",
    "\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            reward = reward if not done or R == 499 else -100\n",
    "\n",
    "            if done :\n",
    "                next_state = None\n",
    "\n",
    "            self.agent.train(state,action,reward,next_state)\n",
    "\n",
    "            state = next_state\n",
    "            R += reward\n",
    "            if done :\n",
    "                break\n",
    "        \n",
    "    def run(self):\n",
    "        while not self.stop_signal:\n",
    "            self.run_episode()\n",
    "    \n",
    "    def stop(self):\n",
    "        self.stop_signal = True\n",
    "                \n",
    "class Optimizer(threading.Thread):\n",
    "    #optimizer를 thread로 여러개돌리기위해 class화\n",
    "    stop_signal = False\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "    def run(self):\n",
    "        while not self.stop_signal:\n",
    "            brain.optimize()\n",
    "                    \n",
    "    def stop(self):\n",
    "        self.stop_signal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = Environment(render=True,game_name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "brain = Brain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brain.load_model('10minute_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "THREADS = 7\n",
    "OPTIMIZERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = [Environment(game_name = name) for i in range(THREADS)]\n",
    "opts = [Optimizer() for i in range(OPTIMIZERS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in envs:\n",
    "    e.start()\n",
    "for o in opts:\n",
    "    o.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in envs:\n",
    "    e.stop()\n",
    "for e in envs:\n",
    "    e.join()\n",
    "for o in opts:\n",
    "    o.stop()\n",
    "for o in opts:\n",
    "    o.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brain.save_model(\"10minute_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-5bec35dc3f6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-3fc04cac46d9>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_signal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-3fc04cac46d9>\u001b[0m in \u001b[0;36mrun_episode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0mgeom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[0mattr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mattr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender1\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m             \u001b[0mglVertex3f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# draw each vertex\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[0mglEnd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_circle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck_glend\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mGLException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No GL context; create a Window first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gl_begin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mGLException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No GL context; create a Window first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gl_begin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglGetError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgluErrorString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
