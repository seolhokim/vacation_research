{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#참고\n",
    "\n",
    "#https://github.com/jaara/AI-blog/blob/master/CartPole-A3C.py 의 코드를\n",
    "#보면서 첨삭하는 식으로 짰습니다.\n",
    "\n",
    "#공부한 사이트\n",
    "'''\n",
    "https://jaromiru.com/2017/03/26/lets-make-an-a3c-implementation/\n",
    "https://www.youtube.com/watch?v=gINks-YCTBs&t=3531s\n",
    "https://github.com/XenderLiu/Policy-Gradient-and-Actor-Critic-Keras/blob/master/agent_dir/agent_actorcritic.py\n",
    "https://github.com/rlcode/reinforcement-learning-kr/blob/master/2-cartpole/2-actor-critic/cartpole_a2c.py#L79\n",
    "http://www.modulabs.co.kr/RL_library/3305\n",
    "https://github.com/rlcode/reinforcement-learning-kr/blob/master/3-atari/1-breakout/breakout_a3c.py\n",
    "'''\n",
    "\n",
    "#gradient관련\n",
    "\n",
    "'''\n",
    "https://wikidocs.net/8281\n",
    "http://blog.naver.com/PostView.nhn?blogId=atelierjpro&logNo=220978848453&parentCategoryNo=&categoryNo=&viewDate=&isShowPopularPosts=false&from=postView\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sh2\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.layers import Dense,merge\n",
    "from keras.models import Sequential ,Input, Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import threading\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "N_STEP_RETURN = 16\n",
    "GAMMA_N = GAMMA ** N_STEP_RETURN\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "reward_size = 1\n",
    "\n",
    "LOSS_V = .5 # v loss coefficient\n",
    "LOSS_ENTROPY = .01 # entropy coefficient\n",
    "MIN_BATCH = 128\n",
    "NONE_STATE = np.zeros(state_size)\n",
    "name = 'CartPole-v1'\n",
    "\n",
    "class Agent:\n",
    "    #environment와 상호작용으로 얻은 data를 memory에 담고\n",
    "    #처리해 brain으로 push\n",
    "    def __init__(self,brain,instance_name):\n",
    "        self.memory = []\n",
    "        self.instance_name = instance_name\n",
    "        \n",
    "        self.R = 0 #이후에 받을수 있는 모든 Reward 는 R, 현상황에 받는 reward는 reward로 표기하였다.\n",
    "        self.model = self.build_local_model()\n",
    "        self.t_max = 20\n",
    "        self.t = 0\n",
    "    \n",
    "    def update_local_model(self):\n",
    "        self.model.set_weights(brain.model.get_weights())\n",
    "        \n",
    "    def build_local_model(self):\n",
    "        input_layer = Input(batch_shape = (None,state_size))\n",
    "        h1 = Dense(24, activation = 'relu',name = 'hidden_layer_01')(input_layer)\n",
    "        h2 = Dense(24, activation = 'relu',name = 'hidden_layer_02')(input_layer)\n",
    "        action = Dense(action_size,activation = 'softmax',name = 'action')(h1)\n",
    "        action_value = Dense(1,activation='linear',name = 'action_value')(h2)\n",
    "        model = Model(inputs = input_layer , outputs=[action,action_value])\n",
    "        model._make_predict_function()\n",
    "        model.set_weights(brain.model.get_weights())\n",
    "        return model\n",
    "    \n",
    "    def act(self,state):\n",
    "        #decaying e를 통해 expliot and exploration는 하지만\n",
    "        #목적함수에 cross entropy로 exploration이 일어나도록 했는데, 또 해야하는지는 의문\n",
    "        #random.choice로 주는 것도 확률을 두번일으키게 하는 것이라, 수정할까 고민함.\n",
    "\n",
    "        state = np.array([state])\n",
    "            \n",
    "        percent,_ = self.model.predict(state) #actor.predict_action(state)\n",
    "        action = np.random.choice(action_size,p=percent[0])\n",
    "        return action\n",
    "    \n",
    "    def train(self, state, action, reward, next_state):\n",
    "        #한번 행동후 sars 모두 memory에 저장,\n",
    "        #Reward 계산.\n",
    "        #game이 done됐을시 get_sample 함수를 통해\n",
    "        #next_state를 수정하며,Reward 계산 후 brain으로 현재 쌓인학습데이터를 모두 보냄\n",
    "        \n",
    "        # N_STEP_RETURN보다 memory의 길이가 길어졌을 시,\n",
    "        # next_state 수정하며,Reward 계산 후, memory 비움\n",
    "        def get_sample(memory,n):\n",
    "            state, action, _, _ =memory[0]\n",
    "            _, _, _, next_state = memory[n-1]\n",
    "            return state, action, self.R, next_state\n",
    "        \n",
    "        action_onehot = np.zeros(action_size)\n",
    "        action_onehot[action] = 1\n",
    "        \n",
    "        self.memory.append((state,action_onehot,reward,next_state))\n",
    "        \n",
    "        self.R = (self.R + reward * GAMMA_N) / GAMMA\n",
    "        \n",
    "        if next_state is None :\n",
    "            \n",
    "            while len(self.memory) > 0 :\n",
    "                n = len(self.memory)\n",
    "                state, action, reward, next_state = get_sample(self.memory,n)\n",
    "                brain.train_push(state, action, reward, next_state)\n",
    "                \n",
    "                self.R = (self.R - self.memory[0][2]) / GAMMA\n",
    "                self.memory.pop(0)\n",
    "                \n",
    "            self.R = 0\n",
    "            \n",
    "        if len(self.memory) >= N_STEP_RETURN : \n",
    "            state, action, reward, next_state = get_sample(self.memory, N_STEP_RETURN)\n",
    "            brain.train_push(state, action, reward, next_state)\n",
    "            \n",
    "            self.R = self.R - self.memory[0][2]\n",
    "            self.memory.pop(0)\n",
    "        if self.t <self.t_max:\n",
    "            self.t += 1\n",
    "        else :\n",
    "            self.t = 0\n",
    "            self.update_local_model()\n",
    "            \n",
    "        #print(\"<\",self.instance_name,\" memory length : \",len(self.memory),\">\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Brain:\n",
    "    '''\n",
    "    각 Environment의 agents 로부터 정보를 받아 한꺼번에 처리함으로서 학습데이터간의 종속성 해결\n",
    "    tensorflow session은 오류검색해서 session등록만 해주고 다 backend K로 통일해 함수사용했음.\n",
    "    \n",
    "    '''\n",
    "    #train_queue는 각 Environment의 agent의 state, action, reward, next_state, next_state의 상태를 담음\n",
    "    train_queue = [ [], [], [], [], [] ]\n",
    "    lock_queue = threading.Lock()\n",
    "    def __init__(self):\n",
    "        #먼저 기존 tensorflow 에서 세션을 만든다. \n",
    "        #그리고 그것을 keras에 등록시켜 keras가 해당 세션을 사용할 수 있도록 한다.\n",
    "        self.session = tf.InteractiveSession()\n",
    "        K.set_session(self.session)\n",
    "        \n",
    "        #직접 만든 tensor를 쓰기위해선 initialization을 해야함\n",
    "        K.manual_variable_initialization(True)\n",
    "        self.actor_lr = 2.5e-4\n",
    "        #policy 를 actor_model을 통해 근사시킴\n",
    "        self.model = self.actor_model()\n",
    "        _,_,_,self.object_function = self.build_object_function(self.model)\n",
    "        #변수를 만들었기 때문에 initializer를 run해줘야함\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.default_graph = tf.get_default_graph()\n",
    "        #self.default_graph.finalize()\n",
    "    def load_model(self,dir):\n",
    "        self.model.load_weights(dir)\n",
    "        \n",
    "    def save_model(self,dir):\n",
    "        self.model.save_weights(dir)\n",
    "        \n",
    "    def actor_model(self):\n",
    "        #actor의 policy를 근사하는 output action과\n",
    "        #critic역할을 하는 action_value를 return\n",
    "        input_layer = Input(batch_shape = (None,state_size))\n",
    "        h1 = Dense(24, activation = 'relu',name = 'hidden_layer_01')(input_layer)\n",
    "        h2 = Dense(24, activation = 'relu',name = 'hidden_layer_02')(input_layer)\n",
    "        action = Dense(action_size,activation = 'softmax',name = 'action')(h1)\n",
    "        action_value = Dense(1,activation='linear',name = 'action_value')(h2)\n",
    "        model = Model(inputs = input_layer , outputs=[action,action_value])\n",
    "        model._make_predict_function()\n",
    "        return model\n",
    "    \n",
    "   \n",
    "    \n",
    "    def build_object_function(self,model):\n",
    "        #placeholder 생성\n",
    "        action_tensor = K.placeholder(dtype = 'float32',shape = [None,action_size])\n",
    "        reward_tensor = K.placeholder(dtype = 'float32',shape=[None,reward_size]) #\n",
    "        #model로부터 action확률과, value를 tensor로 받아옴.\n",
    "        #print(self.model.get_input_at(0))\n",
    "        #print(self.model.get_output_at(0)[0])\n",
    "        #print(self.model.get_output_at(0)[1])\n",
    "        probability, value = model(self.model.get_input_at(0))\n",
    "\n",
    "        #policy = self.model.get_output_at(0)[0]\n",
    "        \n",
    "        #probability가 0이 될까봐 1e-10을 더함\n",
    "        log_probability = K.log(K.sum(action_tensor * probability\\\n",
    "                                             ,axis=1)+1e-10) #,keepdims = True\n",
    "\n",
    "        #reward - base value로 분산을 낮춰줌\n",
    "        advantage = K.sum(reward_tensor - value,axis=1)\n",
    "\n",
    "        #policy 를 근사할땐 advantage gradient의 update는 멈춰줘야함\n",
    "        loss_policy = - (log_probability * K.stop_gradient(advantage))\n",
    "        \n",
    "        #value는 advantage값의 ^2에 계수를 곱\n",
    "        loss_value = ((LOSS_V) * K.square(advantage))\n",
    "        #print(loss_value)\n",
    "        #exploration 을위해 entropy\n",
    "        entropy = ((LOSS_ENTROPY) * K.sum(probability * K.log(probability+1e-10), axis=1)) #, keepdims = True\n",
    "\n",
    "        #최종 loss function\n",
    "\n",
    "        loss_total = (loss_policy + loss_value + entropy)\n",
    "        #print(loss_total)\n",
    "        optimizer = RMSprop(lr=self.actor_lr, decay=.99)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights,[],loss_total)\n",
    "        train = K.function([self.model.get_input_at(0),action_tensor,reward_tensor],[],updates=updates)\n",
    "        return self.model.get_input_at(0), action_tensor, reward_tensor, train\n",
    "    \n",
    "    def optimize(self):\n",
    "        #MIN_BATCH 보다 train_queue가 작으면 return\n",
    "        #MIN_BATCH * 5 보다 처리해야할 양이 많으면 들어오는 batch를 줄이던지,\n",
    "        #optimizer갯수를늘림\n",
    "        #object_function 으로 보냄\n",
    "        \n",
    "        if len(self.train_queue[0]) > 500:\n",
    "            print(\"학습데이터 길이\",len(self.train_queue[0]))\n",
    "        \n",
    "        if len(self.train_queue[0])< MIN_BATCH:\n",
    "            return\n",
    "        with self.lock_queue:\n",
    "            if len(self.train_queue[0]) < MIN_BATCH:# more thread could have passed without lock\n",
    "                return \n",
    "        \n",
    "            state, action, reward, next_state, state_mask = self.train_queue\n",
    "            self.train_queue = [[],[],[],[],[]]\n",
    "        #학습데이터를 np.vstack으로 쌓음\n",
    "        \n",
    "        state = np.vstack(state)\n",
    "        action = np.vstack(action)\n",
    "        reward = np.vstack(reward)\n",
    "        next_state = np.vstack(next_state)\n",
    "        state_mask = np.vstack(state_mask)\n",
    "        \n",
    "\n",
    "        if len(state) > 5 * MIN_BATCH:\n",
    "            print(\"minimizing batch\")\n",
    "        # train data로부터는 N_STEP_RETURN 이후의 value를 못받으므로 더해줘야함\n",
    "        value = self.predict_value(next_state)\n",
    "        \n",
    "        reward = reward + GAMMA_N * value * state_mask\n",
    "        self.object_function([state,action,reward])\n",
    "    \n",
    "    def train_push(self,state, action, reward, next_state):\n",
    "        #train_queue로 학습데이터를 보냄\n",
    "        with self.lock_queue:\n",
    "            self.train_queue[0].append(state)\n",
    "            self.train_queue[1].append(action)\n",
    "            self.train_queue[2].append(reward)\n",
    "\n",
    "            if next_state is None:\n",
    "                self.train_queue[3].append(NONE_STATE)\n",
    "                self.train_queue[4].append(0.)\n",
    "\n",
    "            else :\n",
    "                self.train_queue[3].append(next_state)\n",
    "                self.train_queue[4].append(1.)\n",
    "\n",
    "    def predict(self, state): \n",
    "        with self.default_graph.as_default():\n",
    "            action, value = self.model.predict(state)\n",
    "            return action,value\n",
    "\n",
    "    def predict_action(self, state): \n",
    "        with self.default_graph.as_default():\n",
    "            action, value = self.model.predict(state)\n",
    "            return action\n",
    "\n",
    "    def predict_value(self, state): \n",
    "        with self.default_graph.as_default():\n",
    "            action, value = self.model.predict(state)\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Environment(threading.Thread):\n",
    "    #환경을 thread로 여러개돌리기위해 class로만듬\n",
    "    stop_signal = False\n",
    "    \n",
    "    def __init__(self,game_name,instance_name,brain,render = False):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.instance_name = instance_name\n",
    "        self.render = render\n",
    "        self.env = gym.make(game_name)\n",
    "        self.agent = Agent(brain,instance_name = instance_name)\n",
    "        #self.brain = brain\n",
    "    def run_episode(self):\n",
    "        state = self.env.reset()\n",
    "        R = 0\n",
    "        while True:\n",
    "            if self.render :\n",
    "                self.env.render()\n",
    "            action = self.agent.act(state)\n",
    "\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            reward = reward if not done or R == 499 else -10\n",
    "            #print('state : ',state)\n",
    "            #print('action : ',action)\n",
    "            #print('reward :',reward)\n",
    "            #print('self.agent.R : ',self.agent.R)\n",
    "            if done :\n",
    "                next_state = None\n",
    "\n",
    "            self.agent.train(state,action,reward,next_state)\n",
    "\n",
    "            state = next_state\n",
    "            R += reward\n",
    "            if done :\n",
    "                score_list.append(R)\n",
    "                break\n",
    "            \n",
    "        \n",
    "    def run(self):\n",
    "        while not self.stop_signal:\n",
    "            self.run_episode()\n",
    "    \n",
    "    def stop(self):\n",
    "        self.stop_signal = True\n",
    "                \n",
    "class Optimizer(threading.Thread):\n",
    "    #optimizer를 thread로 여러개돌리기위해 class화\n",
    "    stop_signal = False\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "    def run(self):\n",
    "        while not self.stop_signal:\n",
    "            brain.optimize()\n",
    "                    \n",
    "    def stop(self):\n",
    "        self.stop_signal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n"
     ]
    }
   ],
   "source": [
    "#agent = Agent()\n",
    "brain = Brain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "THREADS = 8\n",
    "OPTIMIZERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = [Environment(game_name = name,brain = brain,instance_name = ('env'+str(i))) for i in range(THREADS)]\n",
    "opts = [Optimizer() for i in range(OPTIMIZERS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in envs:\n",
    "    e.start()\n",
    "for o in opts:\n",
    "    o.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in envs:\n",
    "    e.stop()\n",
    "for e in envs:\n",
    "    e.join()\n",
    "for o in opts:\n",
    "    o.stop()\n",
    "for o in opts:\n",
    "    o.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23f05b5cc18>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "plt.plot(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = Environment(render=True,game_name = name)\n",
    "env_test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
