{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/rlcode/reinforcement-learning-kr/blob/master/2-cartpole/2-actor-critic/cartpole_a2c.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "\n",
    "# 카트폴 예제에서의 액터-크리틱(A2C) 에이전트\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "\n",
    "        # 액터-크리틱 하이퍼파라미터\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_lr = 0.001\n",
    "        self.critic_lr = 0.005\n",
    "\n",
    "        # 정책신경망과 가치신경망 생성\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "        self.actor_updater = self.actor_optimizer()\n",
    "        self.critic_updater = self.critic_optimizer()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.actor.load_weights(\"./save_model/cartpole_actor_trained.h5\")\n",
    "            self.critic.load_weights(\"./save_model/cartpole_critic_trained.h5\")\n",
    "\n",
    "    # actor: 상태를 받아 각 행동의 확률을 계산\n",
    "    def build_actor(self):\n",
    "        actor = Sequential()\n",
    "        actor.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.add(Dense(self.action_size, activation='softmax',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        actor.summary()\n",
    "        return actor\n",
    "\n",
    "    # critic: 상태를 받아서 상태의 가치를 계산\n",
    "    def build_critic(self):\n",
    "        critic = Sequential()\n",
    "        critic.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.add(Dense(self.value_size, activation='linear',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.summary()\n",
    "        return critic\n",
    "\n",
    "    # 정책신경망의 출력을 받아 확률적으로 행동을 선택\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(state, batch_size=1).flatten()\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # 정책신경망을 업데이트하는 함수\n",
    "    def actor_optimizer(self):\n",
    "        action = K.placeholder(shape=[None, self.action_size])\n",
    "        advantage = K.placeholder(shape=[None, ])\n",
    "\n",
    "        action_prob = K.sum(action * self.actor.output, axis=1)\n",
    "        cross_entropy = K.log(action_prob) * advantage\n",
    "        loss = -K.sum(cross_entropy)\n",
    "\n",
    "        optimizer = Adam(lr=self.actor_lr)\n",
    "        updates = optimizer.get_updates(self.actor.trainable_weights, [], loss)\n",
    "        train = K.function([self.actor.input, action, advantage], [],\n",
    "                           updates=updates)\n",
    "        return train\n",
    "\n",
    "    # 가치신경망을 업데이트하는 함수\n",
    "    def critic_optimizer(self):\n",
    "        target = K.placeholder(shape=[None, ])\n",
    "\n",
    "        loss = K.mean(K.square(target - self.critic.output))\n",
    "\n",
    "        optimizer = Adam(lr=self.critic_lr)\n",
    "        updates = optimizer.get_updates(self.critic.trainable_weights, [], loss)\n",
    "        train = K.function([self.critic.input, target], [], updates=updates)\n",
    "\n",
    "        return train\n",
    "\n",
    "    # 각 타임스텝마다 정책신경망과 가치신경망을 업데이트\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        value = self.critic.predict(state)[0]\n",
    "        next_value = self.critic.predict(next_state)[0]\n",
    "\n",
    "        act = np.zeros([1, self.action_size])\n",
    "        act[0][action] = 1\n",
    "\n",
    "        # 벨만 기대 방정식를 이용한 어드벤티지와 업데이트 타깃\n",
    "        if done:\n",
    "            advantage = reward - value\n",
    "            target = [reward]\n",
    "        else:\n",
    "            advantage = (reward + self.discount_factor * next_value) - value\n",
    "            target = reward + self.discount_factor * next_value\n",
    "\n",
    "        self.actor_updater([state, act, advantage])\n",
    "        self.critic_updater([state, target])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CartPole-v1 환경, 최대 타임스텝 수가 500\n",
    "    env = gym.make('CartPole-v1')\n",
    "    # 환경으로부터 상태와 행동의 크기를 받아옴\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # 액터-크리틱(A2C) 에이전트 생성\n",
    "    agent = A2CAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # 에피소드가 중간에 끝나면 -100 보상\n",
    "            reward = reward if not done or score == 499 else -100\n",
    "\n",
    "            agent.train_model(state, action, reward, next_state, done)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # 에피소드마다 학습 결과 출력\n",
    "                score = score if score == 500.0 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/cartpole_a2c.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "                # 이전 10개 에피소드의 점수 평균이 490보다 크면 학습 중단\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                    agent.actor.save_weights(\"./save_model/cartpole_actor.h5\")\n",
    "                    agent.critic.save_weights(\n",
    "                        \"./save_model/cartpole_critic.h5\")\n",
    "                    sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
