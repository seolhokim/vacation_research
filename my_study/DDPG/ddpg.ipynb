{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yes~~!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel:\n",
    "    def __init__(self,state_size,action_size, tau):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.tau = tau\n",
    "        \n",
    "        self.actor = self.build_actor()\n",
    "        self.target_actor = self.build_actor()\n",
    "        \n",
    "    \n",
    "    def build_actor(self):\n",
    "        input_layer = Input(shape =(self.state_size,))\n",
    "        h1 = Dense(24, activation ='relu',kernel_initializer='he_uniform')(input_layer)\n",
    "        h2 = Dense(24, activation ='relu',kernel_initializer='he_uniform')(h1)\n",
    "        h3 = Dense(self.action_size, activation = 'relu',kernel_initializer='he_uniform')(h2)\n",
    "        output = Dense(self.action_size, activation='softmax',kernel_initializer='he_uniform')(h3)\n",
    "        model = Model(inputs =input_layer, outputs = output)\n",
    "        return model,input_layer\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        if(random.random()< self.epsilon):\n",
    "            if(self.epsilon > 0.01):\n",
    "                self.epsilon = self.epsilon * self.decaying_epsilon\n",
    "            \n",
    "            return randint((self.action_size))\n",
    "        \n",
    "        policy= self.target_actor.predict(state) #.flatten()\n",
    "        \n",
    "        return np.random.choice(self.action_size,1,p=policy[0])[0]\n",
    "    \n",
    "    def actor_optimizer(self):\n",
    "        action = K.placeholder(shape =[None, self.action_size])\n",
    "        advantage = K.placeholder(shape=[None,])\n",
    "        action_prob = K.sum(action * self.actor.output,axis = 1)\n",
    "        cross_entropy = K.log(action_prob) * advantage\n",
    "        loss = -K.sum(cross_entropy) #최대화하려면 마이너스?\n",
    "        \n",
    "        optimizer = Adam(lr=self.actor_lr)\n",
    "        updates = optimizer.get_updates(self.actor.trainable_weights,[],loss)\n",
    "        train = K.function([self.actor.input,action,advantage],[],updates=updates)\n",
    "        return train\n",
    "    \n",
    "    def target_actor_train(self):\n",
    "        actor_weights = self.actor.get_weights()\n",
    "        actor_target_weights = self.target_actor.get_weights()\n",
    "        for i in range(len(actor_weights)):\n",
    "            actor_target_weights[i] = self.tau * actor_weights[i] + (1-self.tau) \\\n",
    "            * actor_target_weights[i]\n",
    "        self.target_actor.set_weights(actor_target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_q_values = critic.target_model.predict(\n",
    "                \\[new_states, actor.target_model.predict(new_states)]) \n",
    "a_for_grad = actor.model.predict(states)\n",
    "grads = critic.gradients(states, a_for_grad)\n",
    "actor.train(states, grads)\n",
    "actor.target_train()\n",
    "critic.target_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
