{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frozen world\n",
    "#stocastic model game reinforcement-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register game \n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id = 'FrozenLake-v1',\n",
    "    entry_point = 'gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs = {'map_name' : '4x4', 'is_slippery' :True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set environment\n",
    "game_env = 4 * 4\n",
    "game_act = 4\n",
    "Q = np.zeros((game_env,game_act))\n",
    "dis = 0.99\n",
    "rList = []\n",
    "learning_rate = 0.85\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "for i in range(1000):\n",
    "    state = env.reset()\n",
    "    rAll = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = np.argmax(Q[state,:]+np.random.randn(1,game_act)/(i+1))\n",
    "        new_state, reward, done,_ = env.step(action)\n",
    "        Q[state,action] = (1-learning_rate) * Q[state,action] + \\\n",
    "        learning_rate * (reward + dis * np.max(Q[new_state,:]))\n",
    "        state = new_state\n",
    "        rAll += reward\n",
    "        \n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rList)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.63573309, 76.30931414,  1.5584072 ,  3.485397  ]),\n",
       " array([ 0.25574892,  0.14814089,  0.27308361, 58.95795727]),\n",
       " array([ 0.        ,  0.29908101,  0.82790649, 34.12293878]),\n",
       " array([ 0.26708736,  0.0250111 ,  0.07289212, 21.16395027]),\n",
       " array([85.42267241,  0.1858311 ,  0.13724631,  0.2065846 ]),\n",
       " array([0., 0., 0., 0.]),\n",
       " array([7.44927279e-02, 5.89405999e-03, 2.88397866e+01, 2.41081061e-05]),\n",
       " array([0., 0., 0., 0.]),\n",
       " array([ 0.19080085,  0.12633052,  0.15436135, 89.23286507]),\n",
       " array([ 0.07097766, 52.59756156,  0.62015721,  0.        ]),\n",
       " array([89.09602493,  0.        ,  0.25115736,  0.        ]),\n",
       " array([0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0.]),\n",
       " array([7.09776555e-02, 7.25746527e-02, 8.78565000e+01, 3.82100391e-01]),\n",
       " array([ 0.        , 99.50108065,  0.        ,  0.        ]),\n",
       " array([0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x : x*100,Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(16)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(16)[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Seqeuntial\n",
    "from keras.layers import Dense"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
